# Imports
import time
import csv
from datetime import datetime, timedelta
import numpy as np
import matplotlib.dates as mdates
import matplotlib.pyplot as plt
import pandas as pd
from scipy.optimize import minimize
from misc import *
from dataclasses import dataclass, field
from typing import List, Dict, Any

bottom_avg_names = ['sample_diff_row_temp', 'sample_diff_row_salt'] 
mid_avg_names = ['sample_diff_midrow_temp', 'sample_diff_midrow_salt'] 

def empty() -> None:
    ...

@function_log
@dataclass
class dataset:
  """
  The dataset (*) class is the main repository for all the data involved in the
  analysis of intrusion events into Bedford Basin. This includes:

  - *.data: The output of the ETL_processes script
  - *.dates and .date_stamp: Dates available in different formats
  - *.metadata_intrusions: Overall metadata of individual analysis process
  - *.identification: Results from the intrusion identification class method
                            [MANUAL or IMPORTED]
  - *.analysis: Results from the analysis class [GET- or USE_COEFFICIENTS]

  NOTE: The only required field when initializing is dataset.path which is the
  path of the .pkl file generated by ETL_processes.py
  """
  path : str

  metadata_intrusions : Dict[str, Any] = field(default_factory=dict)
  identification : object = field(default_factory=empty)
  analysis : object = field(default_factory=empty)

  dates_name = 'sample_timestamps'
  
  def __post_init__(self) -> None:
    self.data = import_joblib(self.path)
    self.dates_stamp = self.data[self.dates_name]
    self.dates = timestamp2datetime_lists(self.dates_stamp)

@function_log
@dataclass
class manual_identification:
    """
    The manual_identification (*) class is an identification method that allows 
    you to manually identify intrusion events in time-depth space plots of 
    temperature and salinity. This method includes:

    - *.intrusion_type: Three main types [NORMAL, MID, REVERSE]
    - *.save: Flip to save [ON] or not [OFF] the identified intrusions as .pkl 
    - *.manualID_dates: Dates of the identified intrusions
    - *.table_IDeffects: Table used to record intrusion data in .csv
    - *.effects: The effects of the intrusions identified. Resulting from the 
                 intrusion_data class method

    NOTE: The intrusion_type selection does not affect this class method, rather
    it is used for metadata. The type of intrusions being identified depends on
    the user. Correct selection of intrusion_type is more relevant for MID 
    intrusion events. The results from this class re saved in 
    dataset.identification
    """

    intrusion_type : str
    save : str

    manualID_dates : List[int] = field(default_factory=list)
    table_IDeffects : Dict[str, Any] = field(default_factory=dict)
    effects : object = field(default_factory=empty)

    lin = "-"*6+' ' # For printing purposes
    depth_name = 'sample_depth'
    temp_range = [0,10] 
    salt_range = [30.5,31.5] 
    oxy_range = [0,12]

    def fill_request_info(self, dates) -> None:
        self.dates = dates

        self.uyears  = np.unique([dt.year for dt in self.dates])
        self.manual_input_type = 'MANUAL'
        self.manual_input = 'N/A'

    @staticmethod
    def create_yearly_matrices(selected_data:dict, year_indices:dict[list]) -> dict:
        """Use separated indices from separate_yearly_dates() by year and create matrices
        with the data ready for plotting"""

        temp_dataframe = selected_data['sample_matrix_temp']
        salt_dataframe = selected_data['sample_matrix_salt']
        
        selected_data_temp = temp_dataframe.to_numpy()
        selected_data_salt = salt_dataframe.to_numpy()

        yearly_profiles_temp: dict = {}
        yearly_profiles_salt: dict = {}

        for year, indices in year_indices.items():
            yearly_profile_temp = selected_data_temp[:, indices]
            yearly_profiles_temp[year] = yearly_profile_temp

            yearly_profile_salt = selected_data_salt[:, indices]
            yearly_profiles_salt[year] = yearly_profile_salt

        return yearly_profiles_temp, yearly_profiles_salt

    def separate_yearly_profiles(self, dataset) -> dict[dict]:
        grouped_years = separate_yearly_dates(self.dates)

        # Create dictionary with yearly profiles indices
        by_year_indices = {year: [self.dates.index(dt) for dt in grouped_years[year]] 
                        for year in self.uyears}

        # Extract yearly profiles of temperature and salinity
        yearly_profiles_temp, yearly_profiles_salt = self.create_yearly_matrices(dataset.data, by_year_indices)
            
        return {'Yearly Temp Profile': yearly_profiles_temp, 
                'Yearly Salt Profile': yearly_profiles_salt, 
                'Indices by Year':by_year_indices}

    def plot_year_profiles(self, year_data: dict[dict], yr: int, dataset) -> dict:

        init_date_index = year_data['Indices by Year'][yr][0]
        last_date_index = year_data['Indices by Year'][yr][-1]
        datetime_list = self.dates[init_date_index:last_date_index]

        # Extract specific year data
        fig, axs = plt.subplots(2)
        year_temp_data = year_data['Yearly Temp Profile'][yr]
        year_salt_data = year_data['Yearly Salt Profile'][yr]

        # Temperature Plot
        xmesh,ymesh = np.meshgrid(datetime_list, dataset.data[self.depth_name])
        mesh0 = axs[0].pcolormesh(xmesh,ymesh,year_temp_data[:,:len(ymesh[0,:])], cmap='seismic')
        fig.colorbar(mesh0, ax=axs[0])
        axs[0].invert_yaxis()
        mesh0.set_clim(self.temp_range)
        axs[0].set_xticks([])

        # Salinity Plot
        mesh1 = axs[1].pcolormesh(xmesh,ymesh,year_salt_data[:,:len(ymesh[0,:])], cmap='seismic')
        fig.colorbar(mesh1, ax=axs[1])
        axs[1].invert_yaxis()
        mesh1.set_clim(self.salt_range)
        axs[1].xaxis.set_major_formatter(mdates.DateFormatter("%m"))

        fig.tight_layout()
        axs[0].text(0.02,0.85,str(yr), transform=axs[0].transAxes,fontsize=14,
                    verticalalignment='bottom',horizontalalignment='left',
                    bbox=dict(facecolor='white',alpha=0.5))

        return {
            'Figure':fig,
            'Axes':axs,
            'Mesh':[mesh0,mesh1]
        }

    @staticmethod
    def from_1970(date: int) -> datetime:
        """Converts points selected from plot to datetime"""

        reference_date = datetime(1970, 1, 1)

        delta = timedelta(days=date)
        datetime_obj = reference_date + delta

        return datetime_obj


    def user_intrusion_selection(self,dataset) -> None:
        logger.info(self.lin+'Intrusion identification in progress')

        yearly_profiles = self.separate_yearly_profiles(dataset)
        # Plots Temperature and Salinity profiles for user to select intrusion dates by year
        for yr in self.uyears:
            fig = self.plot_year_profiles(yearly_profiles, 
                                yr, dataset)

            fig['Figure'].canvas.mpl_connect('button_press_event', onclick)

            fig['Figure'].canvas.mpl_connect('key_press_event', onkey)

            plt.show()

        intrusion_dates = list(np.array(get_points())[:,0])
        self.manualID_dates = [self.from_1970(dt) for dt in intrusion_dates]
        
        logger.info(self.lin+'Intrusion identification completed')

    def save_identification(self) -> None:
        man_name = 'manualID_' + self.intrusion_type + str(int(time.time())) + '.pkl'
        save_joblib(self.manualID_dates, man_name)
        self.save = man_name

    def extract(self, dataset) -> None:
        dataset.identification = self
    
    def run(self, dataset):
        self.fill_request_info(dataset.dates)
        self.user_intrusion_selection(dataset)

        if self.save != 'OFF':
            self.save_identification()
        
        self.extract(dataset)

@function_log
@dataclass
class imported_identification:
    """
    The imported_identification (*) class is an identification method that allows 
    you to import the intrusion identification from a .pkl file saved from a 
    previous manual identificatio process. This includes the same attributes as
    the previous identification method, with the addition of:

    - *.manual_input: The path of the input .pkl file
    """

    intrusion_type : str
    manual_input : str
    
    save : str = 'OFF'
    manualID_dates : List[int] = field(default_factory=list)
    table_IDeffects : Dict[str, Any] = field(default_factory=dict)
    effects : object = field(default_factory=empty)

    def fill_request_info(self, dates) -> None:
        self.uyears  = np.unique([dt.year for dt in dates])
        self.manualID_dates = import_joblib(self.manual_input)
        self.manual_input_type = 'IMPORTED'
    
    def extract(self, dataset) -> None:
        dataset.identification = self
    

    def run(self, dataset):
        self.fill_request_info(dataset.dates)
        self.extract(dataset)

@function_log
@dataclass
class intrusion_data:
    """
    The intrusion_data (*) class allows you to retrieve the effects from the 
    intrusion events selected from the original data (datatset.data). This 
    includes:

    - *.manualID_indices: Intrusion events indices
    - *.manualID_temp_effects: Effects on temperature
    - *.manualID_salt_effects: Effects on salt

    NOTE: The results are saved in dataset.identification.effects
    """

    manualID_indices : List[int] = field(default_factory=list)
    manualID_temp_effects : List[int] = field(default_factory=list)
    manualID_salt_effects : List[int] = field(default_factory=list)
    
    def get_original_indices(self, dataset) -> None:
        """Get the indices of the intrusions identified from the main data (self.dates)"""
        comparison_results = date_comparison(dataset.identification.manualID_dates, dataset.dates)
        compared_dates = comparison_results['Matched']
        intrusion_dates = [match[2] for match in compared_dates]
        self.manualID_indices = [i for i, dt1 in enumerate(dataset.dates) for j, dt2 in enumerate(intrusion_dates) if dt1 == dt2]


    def get_intrusion_effects(self, dataset) -> None:
        """Use the date indices from self.dates to identify the effects of those intrusions
        in self.data"""
        self.manualID_temp_effects = dataset.data[bottom_avg_names[0]][self.manualID_indices]
        self.manualID_salt_effects = dataset.data[bottom_avg_names[1]][self.manualID_indices]

        if dataset.identification.intrusion_type.upper() == 'MID':
            # Selecting data based on mid-depts
            self.manualID_temp_effects = dataset.data[mid_avg_names[0]][self.manualID_indices]
            self.manualID_salt_effects = dataset.data[mid_avg_names[1]][self.manualID_indices]

    def extract(self, dataset) -> None:
        dataset.identification.effects = self
    
    def run(self, dataset):
        self.get_original_indices(dataset)
        self.get_intrusion_effects(dataset)
        self.extract(dataset)

@function_log
@dataclass
class intrusion_analysis:
    """
    The intrusion_analysis (*) class encompasses all the analysis methods that
    can be performed on the intrusion data. Where the two main methods are:

    - estimate_coefficients(): Allows you to estimate the optimal coefficients
                                fro intrusion identification using this script
                                [GET_COEFFICIENTS]
    - known_coefficients(): Allows you to evaluate the performance of specific
                            coefficients [USE_COEFFICIENTS]

    This class includes:

    - *.analysis_type: [GET_COEFFICIENTS or USE_COEFFICIENTS]
    - *.coefficients: Used only for [USE_COEFFICIENTS]
    - *.OP_temp_coeff and *.OP_salt_coeff: Optimized coefficients for temperature
                                            and salinity. Used only for 
                                            [GET_COEFFICIENTS]
    - *.OP_performance: Performance of coefficients used based on the 
                        intrusion_id_performance() method
    - *.OP_performance_spec: More specific performance parameters including
                                number of missed, extra, and found intrusion events
    - *.table_coefficients: Table used to record coefficient data in .csv
    - *.table_coefficients_error: Intermediate table used to record specific 
                                    coefficient data in .csv

    NOTE: The results are saved in dataset.analysis
    """
    
    analysis_type : str
    coefficients : List[int]

    OP_temp_coeff : int = field(default_factory=int)
    OP_salt_coeff : int = field(default_factory=int)
    OP_performance : int = field(default_factory=int)
    OP_performance_spec : Dict[str, Any] = field(default_factory=dict)

    table_coefficients : Dict[str, Any] = field(default_factory=dict)
    table_coefficients_error : Dict[str, Any] = field(default_factory=dict)

    
    lin = "-"*6+' ' # For printing purposes
    OF_range = [-1, 1]  # The range of values that the optimization funtion 

    def intrusion_identification(self, lst: list[int], dataset) -> list[datetime]:
        """Uses given coefficients to identify intrusion events. These
        numbers represent the changes in these variables that should be flagged
        as intrusion events based on a depth-average value (either 60-botttom
        for deep, or mid depths that will depend on raw data transformation)"""
        
        temp_intrusion_coeff, salt_intrusion_coeff = lst

        column_avgs_temp = dataset.data[bottom_avg_names[0]]
        column_avgs_salt = dataset.data[bottom_avg_names[1]]

        if dataset.identification.intrusion_type.upper() == 'MID':
            column_avgs_temp = dataset.data[mid_avg_names[0]]
            column_avgs_salt = dataset.data[mid_avg_names[1]]

        intrusion_temp_indices = list(np.where(column_avgs_temp > temp_intrusion_coeff)[0]+1)
        intrusion_salt_indices = list(np.where(column_avgs_salt > salt_intrusion_coeff)[0]+1)

        all_timestamps = pd.DataFrame(dataset.dates_stamp)

        temp_intrusion_dates = all_timestamps.iloc[intrusion_temp_indices]
        salt_intrusion_dates = all_timestamps.iloc[intrusion_salt_indices]

        estimated_intrusion_dates = [value for value in temp_intrusion_dates.values.tolist() 
                                    if value in salt_intrusion_dates.values.tolist()]
        estimated_intrusion_dates = [item for sublist in estimated_intrusion_dates for item in sublist]
        estimated_intrusion_dates = timestamp2datetime_lists(estimated_intrusion_dates)

        return estimated_intrusion_dates


    def intrusion_id_performance(self, init_coeff: list, dataset) -> int:
        """Compares the manually identified intrusion and the estimated intrusions
        to evaluate the coefficient performance"""

        estimated_intrusion_dates = self.intrusion_identification(init_coeff, dataset)
        real_intrusion_dates = dataset.identification.manualID_dates
        comparison_dates = date_comparison(real_intrusion_dates, estimated_intrusion_dates)
            
        missed_id = comparison_dates['Only Manual']
        extra_id = comparison_dates['Only Estimated']

        # Performance Parameters
        if len(estimated_intrusion_dates) != 0:
            missed_id_parameter = len(missed_id)/len(real_intrusion_dates)
            extra_id_parameter = len(extra_id)/len(estimated_intrusion_dates)

            performance_parameter = ((len(real_intrusion_dates) * missed_id_parameter +
                                      len(estimated_intrusion_dates) * extra_id_parameter)/
                                    (len(real_intrusion_dates)+len(estimated_intrusion_dates)))
        else:
            performance_parameter = 1

        return performance_parameter


    def estimate_coefficients(self, dataset) -> None:
        """Estimates optimized coeeficients by iterarting through temperature coefficients
        between self.OF_range and salinity coefficients between 0 to self.OF_range[1]
        and finding the combination with the best results based on a performance parameter"""

        logger.info(self.lin+'Estimating coefficients for optimized intrusion identification')

        real_intrusion_dates = dataset.identification.manualID_dates
        range = self.OF_range
        
        temp_range = np.arange(range[0],range[1],0.025)
        salt_range = np.arange(0,range[1],0.02)
        result_final = []

        # Minimize the performance parameter
        for temp_guess in temp_range:
            for salt_guess in salt_range:
                initial_guess = [temp_guess, salt_guess]
                result = minimize(self.intrusion_id_performance, initial_guess, args = (dataset))
                result_final.append((result.x, result.fun))

        best_coefficients = min(result_final, key= lambda x: x[1])

        self.OP_performance = best_coefficients[1] 
        self.OP_temp_coeff = list(best_coefficients[0])[0] 
        self.OP_salt_coeff = list(best_coefficients[0])[1] 
        
        self.OP_performance_spec = date_comparison(real_intrusion_dates, 
                                                self.intrusion_identification([self.OP_temp_coeff, self.OP_salt_coeff], dataset))
        

    def known_coefficients(self, dataset):

        self.OP_temp_coeff = self.coefficients[0] 
        self.OP_salt_coeff = self.coefficients[1]

        self.OP_performance = self.intrusion_id_performance([dataset,self.coefficients])
        
        self.OP_performance_spec = date_comparison(dataset.identification.manualID_dates, 
                                                self.intrusion_identification(self.coefficients, dataset))

    def extract(self, dataset) -> None:
        dataset.analysis = self

    def run(self, dataset):
        if self.analysis_type.upper() == 'GET_COEFFICIENTS':
            self.estimate_coefficients(dataset)
        else:
            self.known_coefficients(dataset)

        self.extract(dataset)

@function_log
@dataclass
class meta:
    """
    The meta (*) class allows you to record metadata into multiple .csv files.
    This includes:

    - *.table_coefficients_error_comb: Table used to record specific coefficient 
    data in .csv

    NOTE: The results are not saved in dataset
    """

    table_coefficients_error_comb : Dict[str, Any] = field(default_factory=dict)
    
    meta_path = '../data/PROCESSED/TABLES/'
    coeff_error_table = 'coefficients_error.csv'
    coeff_table = 'coefficients.csv'
    intrusions_table = 'intrusionID+effect.csv'
    meta_table = 'metadata_intrusions.csv'

    @staticmethod
    def count_csv_rows(path) -> int:
        """Count number of rows to identify the new recording's index"""
        with open(path,'r') as file:
            read = csv.reader(file)
            row_count = sum(1 for _ in read)

        return row_count
    

    def record_single(self, table, dicts) -> None:
        """Record single row metadata"""
        table_path = self.meta_path+table
        row_num1 = self.count_csv_rows(table_path)

        if row_num1 == 0:
            dicts['ID'] = 1
            dataf= pd.DataFrame(dicts)
            dataf.to_csv(table_path,mode='a', header=True, index=False)
        else:
            dicts['ID'] = row_num1
            dataf= pd.DataFrame(dicts)
            dataf.to_csv(table_path,mode='a', header=False, index=False)

    def integrate_metadata(self, dataset) -> None:
        dataset.metadata_intrusions['Input_dataset'] = dataset.path
        dataset.metadata_intrusions['Current_time'] = time.ctime()
        dataset.metadata_intrusions['Init_year'] = [dataset.identification.uyears[0]]
        # Record Final Year 
        dataset.metadata_intrusions['End_year'] = [dataset.identification.uyears[-1]]
        dataset.metadata_intrusions['Intrusion_type'] = [dataset.identification.intrusion_type]
        dataset.metadata_intrusions['manual_input_type'] = dataset.identification.manual_input_type
        dataset.metadata_intrusions['manual_input_path'] = dataset.identification.manual_input
        dataset.metadata_intrusions['manual_input_save'] = dataset.identification.save 
        dataset.metadata_intrusions['Variables_used'] = str(['salinity', 'temperature']) # Record Variables used

        dataset.identification.table_IDeffects['Dates'] = dataset.identification.manualID_dates
        dataset.identification.table_IDeffects['Index'] = dataset.identification.effects.manualID_indices # Record Intrusion indices
        dataset.identification.table_IDeffects['Temp_effects'] = dataset.identification.effects.manualID_temp_effects # Record Intrusion effects
        dataset.identification.table_IDeffects['Salt_effects'] = dataset.identification.effects.manualID_salt_effects

        dataset.analysis.table_coefficients['Temp_coefficient'] = [dataset.analysis.OP_temp_coeff] # Record Optimized tempewrature coefficient
        dataset.analysis.table_coefficients['Salt_coefficient'] = [dataset.analysis.OP_salt_coeff] # Record Optimized salinity coefficient
        dataset.analysis.table_coefficients['Performance'] = [dataset.analysis.OP_performance] # Record performnace of optimized coefficients

        result_comp = dataset.analysis.OP_performance_spec
        dataset.analysis.table_coefficients_error['Missed'] = result_comp['Only Manual'] # Record Intrusions missed based on manual
        dataset.analysis.table_coefficients_error['Extra'] = result_comp['Only Estimated'] # Record False positives based on manual
        dataset.analysis.table_coefficients_error['Found'] = result_comp['Matched'] # Record Correct identification based on manual
    

    def record_metadata(self, dataset) -> None:
        """Record all the metadata into their corresponding .csv file"""

        row_num = self.count_csv_rows(self.meta_path+self.meta_table)
        rows_intrusion = len(dataset.identification.table_IDeffects['Dates'])

        rows_missed = len(dataset.analysis.table_coefficients_error['Missed'])
        rows_extra = len(dataset.analysis.table_coefficients_error['Extra'])
        rows_found = len(dataset.analysis.table_coefficients_error['Found'])
        self.table_coefficients_error_comb['Type'] = ['Missed']*rows_missed + ['Extra']*rows_extra + ['Found']*rows_found
        self.table_coefficients_error_comb['Dates'] = list(dataset.analysis.table_coefficients_error['Missed']) + list(dataset.analysis.table_coefficients_error['Extra']) + [sub[-1] for sub in list(dataset.analysis.table_coefficients_error['Found'])]
        rows_error = len(self.table_coefficients_error_comb['Dates'])

        if row_num == 0:
            index = 1
            head = True
        else:
            index = row_num
            head = False

        dataset.identification.table_IDeffects['ID'] = [index]*rows_intrusion
        self.table_coefficients_error_comb['Error'] = [index]*rows_error
        dataf_ideffects = pd.DataFrame(dataset.identification.table_IDeffects)
        dataf_error = pd.DataFrame(self.table_coefficients_error_comb)
        dataf_ideffects.to_csv(self.meta_path+self.intrusions_table,mode='a', header=head, index=False)
        dataf_error.to_csv(self.meta_path+self.coeff_error_table,mode='a', header=head, index=False)
        
        self.record_single(self.meta_table, dataset.metadata_intrusions)
        self.record_single(self.coeff_table, dataset.analysis.table_coefficients)
    
    def run(self, dataset):
        self.integrate_metadata(dataset)
        self.record_metadata(dataset) 

def main() -> None:
    logger = create_logger()
    # Get command line arguments
    varsin = {
            'file_name': 'BBMP_salected_data0.pkl',
            'intrusion_type': 'NORMAL',
            'ID_type': 'MANUAL',
            'analysis_type': 'GET_COEFFICIENTS',
            'coefficient_temp': 0.5,
            'coefficient_salt': 0.5,
            'save_manual': 'OFF',
            'manual_input': 'manualID_MID1720009644.pkl'
        }

    file_name, intrusion_type, id_type, analysis_type, coefficient_temp, coefficient_salt, save_manual, manual_input = get_command_line_args(varsin)
    coefficients = [coefficient_temp, coefficient_salt]

    logger.info(f'File: {file_name} - Intrusion Type: {intrusion_type} - ID type: {id_type} - Analysis type: {analysis_type} - Coefficients: {coefficients} - Save Intrusions: {save_manual} - Manual Intrusion Input: {manual_input}')

    path_data = '../data/PROCESSED/'
    file_dirpath = path_data + file_name

    bbmp = dataset(file_dirpath)

    if id_type.upper() == 'MANUAL':
        intrusion_identification = manual_identification(intrusion_type, save_manual)
    else:
        intrusion_identification = imported_identification(intrusion_type, path_data +manual_input)

    intrusion_identification.run(bbmp)
    logger.info(f'Intrusions Identified: {intrusion_identification.manualID_dates}')

    intrusion_effects = intrusion_data()
    intrusion_effects.run(bbmp)

    analysis = intrusion_analysis(analysis_type, coefficients)
    analysis.run(bbmp)

    logger.info(f'Coefficients Used [temp, salt]: {[analysis.OP_temp_coeff, analysis.OP_salt_coeff]} - Performance: {analysis.OP_performance} - # of missed intrusion: {len(analysis.OP_performance_spec['Only Manual'])} - # of extra intrusions: {len(analysis.OP_performance_spec['Only Estimated'])}')

    data_meta = meta()
    data_meta.run(bbmp)


if __name__ == '__main__':
    main()